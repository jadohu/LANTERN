import os
import copy
import json
import time
import math
import random
from tqdm import tqdm
from typing import List, Tuple

import torch
import numpy as np
import torch.nn as nn
from huggingface_hub import hf_hub_download
from transformers.generation.logits_process import LogitsProcessor, LogitsWarper

from .kv_variants.modeling_lumina_mgpt_kv import ChameleonForConditionalGeneration as KVChameleonForConditionalGeneration
from .drafters.cnets_lumina_mgpt import Model
from .drafters.kv_cache import initialize_past_key_values
from .drafters.choices import *

from .configs.configs import EConfig

TOPK=10

class MultiModalLogitsProcessor(LogitsProcessor):

    def __init__(
        self,
        image_next_line_token_id=8803,
        image_end_token_id=8196,
        voc_size=65536,
    ):
        self.image_next_line_token_id = image_next_line_token_id
        self.image_end_token_id = image_end_token_id

        self.vocab_list = [i for i in range(voc_size)]
        self.image_token_list = [i for i in range(4, 8195 + 1)]
        self.suppress_tokens = torch.tensor(
            [x for x in self.vocab_list if x not in self.image_token_list], device="cuda"
        )

        self.vocab_tensor = torch.arange(voc_size, device="cuda")
        self.suppress_token_mask = torch.isin(self.vocab_tensor, self.suppress_tokens)

    def __call__(self, scores, h_latent_dim=48, w_latent_dim=48,
                    image_start_token_id_index=None, position_ids=None):
        # inputs:
        #   - scores: [seq_len, vocab_size]
        #   - image_start_token_id_index: []
        #   - position_ids: [seq_len]

        if position_ids is None:
            return scores

        # num_generated_image_tokens indicates the number of pure image tokens
        # (i.e., number of tokens after 8197, 8828, 8828) generated so far
        if image_start_token_id_index is None:
            # position_ids over only the image tokens
            num_generated_image_tokens = position_ids - 2  # [seq_len]
        else:
            # position_ids over the whole tokens
            # note that we add 1 to the image_start_token_id_index to account for the
            # shift in the input_ids
            num_generated_image_tokens = position_ids - (image_start_token_id_index + 1 + 2)
        
        # hence the num_generated_image_token + 1 indicates the position of the token
        # generated by the given `scores`
        
        image_constrained_mask = ((num_generated_image_tokens + 1) % (w_latent_dim + 1)) != 0
        if image_constrained_mask.sum() > 0:
            image_constrained_scores = torch.where(self.suppress_token_mask, -float('inf'), scores)
            scores = torch.where(image_constrained_mask[:, None], image_constrained_scores, scores)

        new_line_mask = ((num_generated_image_tokens + 1) % (w_latent_dim + 1)) == 0
        if new_line_mask.sum() > 0:
            new_line_constrained_scores = torch.full_like(scores, -math.inf)
            new_line_constrained_scores[:, self.image_next_line_token_id] = 0
            scores = torch.where(new_line_mask[:, None], new_line_constrained_scores, scores)

        eos_image_mask = (num_generated_image_tokens + 1) == (w_latent_dim + 1) * h_latent_dim + 1
        if eos_image_mask.sum() > 0:
            eos_image_constrained_scores = torch.full_like(scores, -math.inf)
            eos_image_constrained_scores[:, self.image_end_token_id] = 0
            scores = torch.where(eos_image_mask[:, None], eos_image_constrained_scores, scores)

        return scores
        
class InterleavedTopKLogitsWarper(LogitsWarper):
    r"""
    [`LogitsWarper`] that performs top-k, i.e. restricting to the k highest probability elements. Often used together
    with [`TemperatureLogitsWarper`] and [`TopPLogitsWarper`].
    """

    def __init__(
        self,
        image_top_k: int = 2000,
        filter_value: float = -float("Inf"),
        min_tokens_to_keep: int = 1,
    ):
        if not isinstance(image_top_k, int) or image_top_k <= 0:
            raise ValueError(f"`image_top_k` has to be a strictly positive integer, but is {image_top_k}")

        self.image_top_k = max(image_top_k, min_tokens_to_keep)
        self.filter_value = filter_value

    def __call__(self, scores: torch.FloatTensor) -> torch.FloatTensor:  
        top_k = min(self.image_top_k, scores.size(-1))
        
        # Remove all tokens with a probability less than the last token of the top-k
        indices_to_remove = scores < torch.topk(scores, top_k)[0][..., -1, None]
        scores_processed = scores.masked_fill(indices_to_remove, self.filter_value)
        return scores_processed

def pad_path(path: List[int], length: int, pad_value: int = -2) -> List[int]:
    """
    Pad the given path list with a specific value up to a specified length.

    Parameters:
    - path (list): The original list that needs padding.
    - length (int): The desired length of the padded list.
    - pad_value (optional, default=-2): The value to use for padding.

    Returns:
    - list: A new list based on the original path but padded to the desired length.

    Example:
    >>> pad_path([1,2,3], 5)
    [1, 2, 3, -2, -2]

    Note:
    If the given path is already longer than the specified length,
    then no padding occurs, and the original path is returned.
    """

    # Calculate the number of padding values needed by subtracting the length
    # of the path from the desired length.
    # Append the padding values to the original path and return the new list.
    return path + [pad_value] * (length - len(path))

def generate_tree_buffers(tree_choices, device="cuda"):
    sorted_tree_choices = sorted(tree_choices, key=lambda x: (len(x), x))
    tree_len = len(sorted_tree_choices) + 1

    # Initialize depth_counts to keep track of how many choices have a particular depth
    depth_counts = []
    prev_depth = 0
    for path in sorted_tree_choices:
        depth = len(path)
        if depth != prev_depth:
            depth_counts.append(0)
        depth_counts[depth - 1] += 1
        prev_depth = depth

    tree_attn_mask = torch.eye(tree_len, tree_len)
    tree_attn_mask[:, 0] = 1
    start = 0
    for i in range(len(depth_counts)):
        for j in range(depth_counts[i]):
            cur_tree_choice = sorted_tree_choices[start + j]
            # retrieve ancestor position
            if len(cur_tree_choice) == 1:
                continue
            ancestor_idx = []
            for c in range(len(cur_tree_choice) - 1):
                ancestor_idx.append(sorted_tree_choices.index(cur_tree_choice[:c + 1]) + 1)
            tree_attn_mask[j + start + 1, ancestor_idx] = 1
        start += depth_counts[i]

    tree_indices = torch.zeros(tree_len, dtype=torch.long)
    p_indices = [0 for _ in range(tree_len - 1)]
    b_indices = [[] for _ in range(tree_len - 1)]
    tree_indices[0] = 0
    start = 0
    bias = 0
    for i in range(len(depth_counts)):
        inlayer_bias = 0
        b = []
        for j in range(depth_counts[i]):
            cur_tree_choice = sorted_tree_choices[start + j]
            cur_parent = cur_tree_choice[:-1]
            if j != 0:
                if cur_parent != parent:
                    bias += 1
                    inlayer_bias += 1
                    parent = cur_parent
                    b = []
            else:
                parent = cur_parent
            tree_indices[start + j + 1] = cur_tree_choice[-1] + TOPK * (i + bias) + 1
            p_indices[start + j] = inlayer_bias
            if len(b) > 0:
                b_indices[start + j] = copy.deepcopy(b)
            else:
                b_indices[start + j] = []
            b.append(cur_tree_choice[-1] + TOPK * (i + bias) + 1)
        start += depth_counts[i]

    p_indices = [-1] + p_indices
    tree_position_ids = torch.zeros(tree_len, dtype=torch.long)
    start = 0
    for i in range(len(depth_counts)):
        tree_position_ids[start + 1: start + depth_counts[i] + 1] = i + 1
        start += depth_counts[i]

    retrieve_indices_nest = []
    retrieve_paths = []
    for i in range(len(sorted_tree_choices)):
        cur_tree_choice = sorted_tree_choices[-i - 1]
        retrieve_indice = []
        if cur_tree_choice in retrieve_paths:
            continue
        else:
            for c in range(len(cur_tree_choice)):
                retrieve_indice.append(sorted_tree_choices.index(cur_tree_choice[:c + 1]))
                retrieve_paths.append(cur_tree_choice[:c + 1])
        retrieve_indices_nest.append(retrieve_indice)
    max_length = max([len(x) for x in retrieve_indices_nest])
    retrieve_indices = [pad_path(path, max_length) for path in retrieve_indices_nest]
    retrieve_indices = torch.tensor(retrieve_indices, dtype=torch.long)
    retrieve_indices = retrieve_indices + 1
    retrieve_indices = torch.cat([torch.zeros((retrieve_indices.shape[0], 1), dtype=torch.long), retrieve_indices],
                                dim=1)

    maxitem = retrieve_indices.max().item() + 5

    def custom_sort(lst):
        # sort_keys=[len(list)]
        sort_keys = []
        for i in range(len(lst)):
            sort_keys.append(lst[i] if lst[i] >= 0 else maxitem)
        return sort_keys

    retrieve_indices = retrieve_indices.tolist()
    retrieve_indices = sorted(retrieve_indices, key=custom_sort)
    retrieve_indices = torch.tensor(retrieve_indices, dtype=torch.long)

    p_indices = torch.tensor(p_indices)
    p_indices_new = p_indices[retrieve_indices]
    p_indices_new = p_indices_new.tolist()

    b_indices = [[]] + b_indices
    b_indices_new = []
    for ib in range(retrieve_indices.shape[0]):
        iblist = []
        for jb in range(retrieve_indices.shape[1]):
            index = retrieve_indices[ib, jb]
            if index == -1:
                iblist.append([])
            else:
                b = b_indices[index]
                if len(b) > 0:
                    bt = []
                    for bi in b:
                        bt.append(torch.where(tree_indices == bi)[0].item())
                    iblist.append(torch.tensor(bt, device=device))
                else:
                    iblist.append(b)
        b_indices_new.append(iblist)

    # Aggregate the generated buffers into a dictionary
    tree_buffers = {
        "tree_attn_mask": tree_attn_mask.unsqueeze(0).unsqueeze(0),
        "tree_indices": tree_indices,
        "tree_position_ids": tree_position_ids,
        "retrieve_indices": retrieve_indices,
    }

    # Move the tensors in the dictionary to the specified device
    tree_buffers = {
        k: v.clone().to(device)
        if isinstance(v, torch.Tensor)
        else torch.tensor(v, device=device)
        for k, v in tree_buffers.items()
    }
    tree_buffers["p_indices"] = p_indices_new
    tree_buffers["b_indices"] = b_indices_new
    return tree_buffers

class EaLumina_mGPT(nn.Module):

    def __init__(
            self,
            base_model,
            base_model_name_or_path,
            ea_model_path,
            total_token,
            depth,
            top_k,
            threshold,
            cfg_mode,
            eagle_version,
            dtype,
            ea_layer_state_dict
    ):

        super().__init__()
        self.base_model = base_model
        self.config = base_model.config
        self.dtype = dtype
        self.hidden_size = base_model.lm_head.weight.shape[-1]
        self.vocab_size = base_model.lm_head.weight.shape[0]
        self.base_model_name_or_path = base_model_name_or_path
        self.cfg_mode = cfg_mode
        self.eagle_version = eagle_version
        
        config = EConfig.from_pretrained(ea_model_path)
        with open(ea_model_path,"r") as f:
            con=json.loads(f.read())
        try:
            bias=con["bias"]
        except:
            bias=True
        self.ea_layer = Model(config, 
                                bias=bias,
                                total_tokens=total_token,
                                depth=depth,
                                top_k=top_k,
                                threshold=threshold
                        )

        self.nearest_latents = np.load("ckpts/lumina_mgpt/vq_distances/top_8191_indices.npy")
        self.image_token_offset = 4 # image token offset; image tokens are from 4 to 8195
        self.image_tokens = torch.arange(4, 8196, device="cuda")
        self.image_syntax_tokens = torch.tensor([8196, 8197, 8803, 8828], device="cuda")
        self.image_start_token_id = 8197

        low_memory=False

        device = base_model.model.layers[-1].self_attn.q_proj.weight.device
        if device!=base_model.lm_head.weight.device:
            self.ea_layer.diff_device = True
            if not low_memory:
                self.ea_layer.headweight = base_model.lm_head.weight.clone().to(device)
            else:
                self.ea_layer.layer_device = device

        else:
            self.ea_layer.diff_device = False
        self.ea_layer.load_state_dict(ea_layer_state_dict, strict=True)
        self.ea_layer.to(self.dtype).to(device)
        self.ea_layer.init_tree()

        # preemptive initialization of the logits processors since MultiModalLogitsProcessor requires significant time to initialize
        self.internal_logits_processors = [
            MultiModalLogitsProcessor(),
        ]
        self.drafter_logits_processors = copy.deepcopy(self.internal_logits_processors)

    @classmethod
    def from_pretrained(
            cls,
            Type="LLaMA",
            base_model_path=None,
            ea_model_path=None,
            total_token=59,
            depth=5,
            top_k=10,
            threshold=1.0,
            cfg_mode="sequential",
            eagle_version=1,
            dtype=torch.bfloat16,
            device_map="cuda",
            **kwargs,
    ):
        base_model = KVChameleonForConditionalGeneration.from_pretrained(
            base_model_path, torch_dtype=dtype, device_map=device_map, **kwargs
        )

        configpath=os.path.join(ea_model_path, "config.json")
        if not os.path.exists(configpath):
            configpath = hf_hub_download(ea_model_path, "config.json")

        try:
            load_model_path=os.path.join(ea_model_path, "pytorch_model.bin")
            if not os.path.exists(load_model_path):
                load_model_path=hf_hub_download(ea_model_path, "pytorch_model.bin")
            ea_layer_state_dict = torch.load(load_model_path,
                                             map_location=base_model.device)
        except:
            from safetensors.torch import load_file
            load_model_path = os.path.join(ea_model_path, "model.safetensors")
            if not os.path.exists(load_model_path):
                load_model_path = hf_hub_download(ea_model_path, "model.safetensors")
            ea_layer_state_dict = load_file(load_model_path)
            
        model = cls(
            base_model,
            base_model_path,
            configpath,
            total_token,
            depth,
            top_k,
            threshold,
            cfg_mode,
            eagle_version,
            dtype,
            ea_layer_state_dict
        )

        model.ea_layer = model.ea_layer.to(dtype)
        if device_map == "cuda":
            model.ea_layer = model.ea_layer.cuda()

        if total_token==-1:
            device = model.base_model.model.layers[0].self_attn.q_proj.weight.device
            cans=[40,48,50,56,60]
            x=[1,1.05,1.07,1.1,1.13]
            times=[]

            for i in range(len(cans)):
                length = cans[i]
                input_ids = torch.randint(0, model.config.vocab_size - 200, (1, length)).to(device)
                torch.cuda.synchronize()
                start_time = time.time()
                for _ in range(20):
                    torch.cuda.synchronize()
                    with torch.no_grad():
                        outputs = model.base_model(input_ids)
                    torch.cuda.synchronize()
                torch.cuda.synchronize()
                end_time = time.time()
                times.append((end_time - start_time) / x[i])
            total_token=cans[times.index(min(times))]
            model.ea_layer.total_tokens=total_token-1

        return model
    
    def forward(
            self,
            input_ids=None,
            attention_mask=None,
            past_key_values=None,
            output_orig=False,
            position_ids=None,
    ):

        with torch.inference_mode():
            # Pass input through the base model
            outputs = self.base_model.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                past_key_values=past_key_values,
                position_ids=position_ids,
            )
            if output_orig:
                orig = self.base_model.lm_head(outputs[0])
            hidden_states = outputs[0]

        if output_orig:
            return outputs, orig, hidden_states
        else:
            return outputs, hidden_states

    def reset_tree_mode(self):
        self.base_model.model.tree_mode = True
        self.base_model.model.tree_mask = None
    
    def initialize_tree(self, input_ids, past_key_values, logits_processors,
                        attention_mask=None, tree_attn_mask=None):
        if self.cfg_mode == "parallel":
            # NOTE : position_ids will be set automatically according to the attn_mask as position_ids = attn_mask.cumsum(dim=-1) - 1
            _, logits, hidden_states = self(
                input_ids=input_ids,
                attention_mask=attention_mask,
                past_key_values=past_key_values,
                output_orig=True,
            )

            logits, uncond_logits = torch.split(logits, [1, 1])
            hidden_states, uncond_hidden_states = torch.split(hidden_states, [1, 1])

            # reduces the input_ids to a single tensor
            input_ids = input_ids[:-1] # [1, seq_len]
        else:
            # For sequential CFG, we don't need to pass attention_mask since we manually separated the input_ids
            # However, note that we need to pass attention_mask to the drafter forward (topK_generate) since it
            # uses parallel CFG regardless of the CFG mode.
            _, logits, hidden_states = self(
                input_ids=input_ids,
                past_key_values=past_key_values["cond"],
                output_orig=True
            )

            self.image_start_token_id_index = torch.where(input_ids[0] == 8197)[0][-1].item()
            uncond_input_ids = input_ids[:, self.image_start_token_id_index:]
            _, uncond_logits, uncond_hidden_states = self(
                input_ids=uncond_input_ids,
                past_key_values=past_key_values["uncond"],
                output_orig=True
            )

        cfg_logits = uncond_logits[:, -1] + self.cfg_scale * (logits[:, -1] - uncond_logits[:, -1])
        for logits_processor in logits_processors[1:]:
            # NOTE : `input_ids[0]` produces better images but `input_ids` is the correct way to do it
            cfg_logits = logits_processor(input_ids, cfg_logits)

        probabilities = torch.nn.functional.softmax(cfg_logits, dim=-1)
        token = torch.multinomial(probabilities, 1)

        input_ids = torch.cat((input_ids, token.to(input_ids.device)), dim=1)
        
        if self.eagle_version == 1:
            self.ea_layer.init_tree(self.tree_choices)
            self.base_model.model.tree_mask = tree_attn_mask
        else:
            self.ea_layer.init_tree()
        
        # use internal logits processors for drafter
        output = self.ea_layer.topK_generate(
            hidden_states=hidden_states,
            uncond_hidden_states=uncond_hidden_states,
            input_ids=input_ids,
            attention_mask=attention_mask,
            head=self.base_model.lm_head,
            logits_processors=self.drafter_logits_processors,
            tree_type="static" if self.eagle_version == 1 else "dynamic",
        )

        if self.eagle_version == 1:
            return output, token
        else:
            return output

    # Only for EAGLE v1
    def generate_candidates(self, tree_logits, tree_indices, retrieve_indices, sample_token):
        sample_token = sample_token.to(tree_indices.device)
        
        candidates_logit = sample_token[0]
        candidates_tree_logits = tree_logits[0]
        candidates = torch.cat([candidates_logit, candidates_tree_logits.view(-1)], dim=-1)

        tree_candidates = candidates[tree_indices]
        tree_candidates_ext = torch.cat(
            [tree_candidates, torch.zeros((1), dtype=torch.long, device=tree_candidates.device) - 1],
            dim = 0
        )

        cart_candidates = tree_candidates_ext[retrieve_indices]

        candidates_tree_prob = tree_logits[1]
        candidates_prob = torch.cat(
            [torch.ones(1, device=candidates_tree_prob.device, dtype=torch.float32), candidates_tree_prob.view(-1)],
            dim=-1
        )

        tree_candidates_prob = candidates_prob[tree_indices]
        tree_candidates_prob_ext = torch.cat(
            [tree_candidates_prob, torch.ones((1), dtype=torch.float32, device=tree_candidates_prob.device)],
            dim = 0
        )
        cart_candidates_prob = tree_candidates_prob_ext[retrieve_indices]

        tree_candidates = tree_candidates.unsqueeze(0)
        return cart_candidates, cart_candidates_prob, tree_candidates

    def tree_decoding(self, tree_candidates, attention_mask, past_key_values, tree_position_ids, 
                        input_ids, retrieve_indices):
        
        position_ids = tree_position_ids + input_ids.shape[1]
        
        if self.cfg_mode == "parallel":
            # replicate the input_ids for the parallel CFG
            tree_candidates = torch.cat((tree_candidates, tree_candidates), dim=0)
            position_ids = torch.cat((position_ids[None], position_ids[None] - self.image_start_token_id_index), dim=0)

            _, tree_logits, hidden_states = self(
                input_ids=tree_candidates,
                attention_mask=attention_mask,
                output_orig=True,
                past_key_values=past_key_values,
                position_ids=position_ids,
            )

            tree_logits, uncond_tree_logits = torch.split(tree_logits, [1, 1])
            hidden_states, uncond_hidden_states = torch.split(hidden_states, [1, 1])

            # reduce the position_ids to a single tensor
            position_ids = position_ids[0]
        
        else:
            # For sequential CFG, we don't need to pass attention_mask since the input_ids are already separated
            _, tree_logits, hidden_states = self(
                input_ids=tree_candidates,
                output_orig=True,
                past_key_values=past_key_values["cond"],
                position_ids=position_ids,
            )

            # unconditional logits
            _, uncond_tree_logits, uncond_hidden_states = self(
                input_ids=tree_candidates,
                output_orig=True,
                past_key_values=past_key_values["uncond"],
                position_ids=position_ids - self.image_start_token_id_index,
            )

        cfg_tree_logits = uncond_tree_logits + self.cfg_scale * (tree_logits - uncond_tree_logits)

        # MultiModalLogitsProcessor
        cfg_tree_logits = self.internal_logits_processors[0](
            cfg_tree_logits[0], image_start_token_id_index=self.image_start_token_id_index, position_ids=position_ids + 1
        )

        # InterleavedTopKLogitsWarper
        cfg_tree_logits = self.internal_logits_processors[1](cfg_tree_logits)
        
        logits = cfg_tree_logits[retrieve_indices]
        return logits, hidden_states, uncond_hidden_states

    def evaluate_posterior(self, logits, candidates, cart_candidates_prob=None, original_prob=None,
                            p_indices=None, tree_candidates=None, b_indices=None,
                            do_sample=True, lantern=False, lantern_k=1000, lantern_delta=0.1):
        if do_sample:
            accept_length = 1
            accept_cand = candidates[0][:1]
            best_candidate = 0

            if self.eagle_version == 1:
                assert cart_candidates_prob is not None, "Cartesian candidate probabilities are required for EAGLE v1"
                assert original_prob is not None, "Original probabilities are required for EAGLE v1"
                assert tree_candidates is not None, "Tree candidates are required for EAGLE v1"
                assert p_indices is not None, "Parent indices are required for EAGLE v1"
                assert b_indices is not None, "B indices are required for EAGLE v1"
                
                cart_candidates_prob = cart_candidates_prob.to(logits.device)

            # for-loop over levels
            for i in range(1, candidates.shape[1]):
                if i != accept_length:
                    break
                
                adjustflag = False
                is_eq = (candidates[:, :accept_length] == accept_cand).all(dim=1)
                fi = torch.nonzero(is_eq, as_tuple=True)[0][0]
                
                gt_logits = logits[fi, i-1]
                gtp = torch.softmax(gt_logits, dim=0)
                
                candidates_set = []

                # for-loop within a level
                for j in range(candidates.shape[0]):
                    if is_eq[j]:
                        x = candidates[j, i]
                        xi = x.item()

                        if xi in candidates_set or xi == -1:
                            continue
                        
                        candidates_set.append(xi)

                        r = random.random()
                        px = gtp[xi]
                        if xi in self.image_syntax_tokens:
                            # accept immediately
                            px = 1.0
                        elif not xi in self.image_tokens:
                            # reject immediately
                            px = 0.0
                        else:
                            if lantern:
                                nearest_probs = gtp[self.nearest_latents[xi - self.image_token_offset, :lantern_k]+self.image_token_offset].reshape(lantern_k, 1)
                                cumsum_nearest_probs = torch.cumsum(nearest_probs, dim=0)

                                if lantern_delta > 1.0:
                                    indices = (cumsum_nearest_probs <= (lantern_delta - 1) * px).nonzero(as_tuple=True)[0]
                                else:
                                    indices = (cumsum_nearest_probs <= lantern_delta).nonzero(as_tuple=True)[0]
                                
                                if indices.numel() == 0:
                                    indices = -1
                                else:
                                    indices = indices[-1]
                                if indices == -1:
                                    px = px
                                else:
                                    px = px + cumsum_nearest_probs[indices]
                        
                        if self.eagle_version == 1:
                            qx = cart_candidates_prob[j, i]
                            if qx <= 0:
                                continue
                        else:
                            qx = 1.0
                        
                        acp = px / qx

                        if r <= acp:
                            accept_cand = torch.cat((accept_cand, x[None]), dim=0)
                            accept_length += 1
                            best_candidate = j
                            break
                        else:
                            assert not xi in self.image_syntax_tokens, "Image syntax tokens should not be rejected"
                            
                            if self.eagle_version == 1:
                                q = original_prob[i - 1][p_indices[j][i]].clone()
                                b = b_indices[j][i]
                                if len(b) > 0:
                                    mask = tree_candidates[0][b]
                                    q[mask] = 0
                                    q = q / q.sum()
                            
                            if lantern and (xi in self.image_tokens):
                                if (indices != -1):
                                    gtp[self.nearest_latents[xi-self.image_token_offset, :lantern_k+1]+self.image_token_offset] = 0
                            
                            if self.eagle_version == 1:
                                gtp = gtp - q
                                gtp[gtp < 0] = 0
                            else:
                                gtp[xi] = 0

                            if gtp.sum() == 0:
                                gtp = torch.ones_like(gtp)
                            
                            gtp /= gtp.sum()
                            adjustflag = True

            if adjustflag and accept_length != candidates.shape[1]:
                sample_p = gtp
            else:
                gt_logits = logits[best_candidate, accept_length-1]
                sample_p = torch.softmax(gt_logits, dim=0)
            
            return torch.tensor(best_candidate), accept_length-1, sample_p
        
        else:
            raise NotImplementedError("Greedy decoding is not implemented yet")

    def update_inference_inputs(self, input_ids, attention_mask,candidates, best_candidate, accept_length,
                                retrieve_indices, do_sample, new_token, past_key_values_data,
                                current_length_data, hidden_states_new, uncond_hidden_states_new, sample_p):
        
        if self.cfg_mode == "parallel":
            prev_input_len = input_ids.shape[1]
            selected_indices = (
                retrieve_indices[best_candidate, : accept_length + 1] + prev_input_len
            )

            for data in past_key_values_data:
                tgt = data[..., selected_indices.to(data.device), :]
                dst = data[..., prev_input_len: prev_input_len + tgt.shape[-2], :]
                dst.copy_(tgt, non_blocking=True)
            
            current_length_data.fill_(prev_input_len + tgt.shape[-2])

            input_ids = torch.cat(
                [input_ids[None, 0], candidates[None, best_candidate, : accept_length + 1].to(input_ids.device)], dim=-1
            )

        else:
            for key in ["cond", "uncond"]:
                if key == "cond":
                    prev_input_len = input_ids.shape[1]
                else:
                    prev_input_len = input_ids.shape[1] - self.image_start_token_id_index
                
                selected_indices = (
                    retrieve_indices[best_candidate, : accept_length + 1] + prev_input_len
                )

                for data in past_key_values_data[key]:
                    tgt = data[..., selected_indices.to(data.device), :]
                    dst = data[..., prev_input_len: prev_input_len + tgt.shape[-2], :]
                    dst.copy_(tgt, non_blocking=True)
                current_length_data[key].fill_(prev_input_len + tgt.shape[-2])
            
            input_ids = torch.cat(
                [input_ids, candidates[None, best_candidate, : accept_length + 1].to(input_ids.device)], dim=-1
            )

        retrieve_hidden_states_new = hidden_states_new[:, retrieve_indices]
        accept_hidden_states_new = retrieve_hidden_states_new[:, best_candidate, : accept_length + 1]

        retrieve_uncond_hidden_states_new = uncond_hidden_states_new[:, retrieve_indices]
        accept_uncond_hidden_states_new = retrieve_uncond_hidden_states_new[:, best_candidate, : accept_length + 1]

        prob = sample_p
        if do_sample:
            token = torch.multinomial(prob, 1)
            token = token[None]
        else:
            token = torch.argmax(prob)
            token = token[None, None]
        
        output = self.ea_layer.topK_generate(
            hidden_states=accept_hidden_states_new,
            uncond_hidden_states=accept_uncond_hidden_states_new,
            input_ids=torch.cat((input_ids, token.to(input_ids.device)), dim=-1),
            attention_mask=attention_mask,
            head=self.base_model.lm_head,
            logits_processors=self.drafter_logits_processors,
            tree_type="static" if self.eagle_version == 1 else "dynamic",
        )

        new_token += accept_length + 1

        return input_ids, output, new_token, token

    @torch.no_grad()
    def generate(
        self,
        input_ids,
        do_sample=True,
        max_new_tokens=2353,
        max_length=4096,
        cfg_scale=3.0,
        top_k=2000,
        logits_processors=None,
        eos_token_ids=None,
        lantern=False,
        lantern_k=1000,
        lantern_delta=0.1,
        tree_choices=mc_sim_7b_63,
        **kwargs
    ):
        # initialize the logits processors
        self.cfg_scale = cfg_scale
        self.ea_layer.cfg_scale = cfg_scale
        
        self.internal_logits_processors.append(
            InterleavedTopKLogitsWarper(image_top_k=top_k)
        )
        if "drafter_top_k" in kwargs and kwargs["drafter_top_k"] is not None:
            self.drafter_logits_processors.append(
                InterleavedTopKLogitsWarper(image_top_k=kwargs["drafter_top_k"])
            )
        else:
            self.drafter_logits_processors.append(
                InterleavedTopKLogitsWarper(image_top_k=top_k)
            )
        
        # manually set the first three tokens since it is fixed for all images
        image_start_sequence = torch.tensor([[8197, 8828, 8828]], dtype=torch.long).to(input_ids.device)
        input_ids = torch.cat((input_ids, image_start_sequence), dim=-1)

        self.eval()
        accept_length_list = []
        
        input_ids = input_ids.clone()
        self.ea_layer.reset_kv()

        if self.eagle_version == 1:
            if hasattr(self, "tree_choices") and self.tree_choices == tree_choices:
                tree_buffers = self.tree_buffers
            else:
                tree_buffers = generate_tree_buffers(
                    tree_choices, device=self.base_model.model.layers[-1].self_attn.q_proj.weight.device
                )
                tree_buffers["retrieve_indices_head"] = tree_buffers["retrieve_indices"].to(self.base_model.lm_head.weight.device)

                if self.cfg_mode == "parallel":
                    # double the tree buffers for parallel mode
                    tree_buffers["tree_attn_mask"] = torch.cat((tree_buffers["tree_attn_mask"], tree_buffers["tree_attn_mask"]), dim=0)

            self.tree_buffers = tree_buffers
            self.tree_choices = tree_choices
        
        if hasattr(self, "past_key_values"):
            if self.cfg_mode == "parallel":
                past_key_values = self.past_key_values
                past_key_values_data = self.past_key_values_data
                current_length_data = self.current_length_data

                # Reset the past key and value states
                current_length_data.zero_()

            else:
                # keep the past key values for the conditional and unconditional inputs separately
                past_key_values, past_key_values_data, current_length_data = {}, {}, {}
                for key in ["cond", "uncond"]:
                    past_key_values[key] = self.past_key_values[key]
                    past_key_values_data[key] = self.past_key_values_data[key]
                    current_length_data[key] = self.current_length_data[key]
                    
                    # Reset the past key and value states
                    current_length_data[key].zero_()

        else:
            if self.cfg_mode == "parallel":
                (past_key_values, past_key_values_data, current_length_data) = initialize_past_key_values(self.base_model, batch_size=2)
                self.past_key_values = past_key_values
                self.past_key_values_data = past_key_values_data
                self.current_length_data = current_length_data

            else:
                self.past_key_values, self.past_key_values_data, self.current_length_data = {}, {}, {}
                past_key_values, past_key_values_data, current_length_data = {}, {}, {}
                
                for key in ["cond", "uncond"]:
                    (past_key_values[key], past_key_values_data[key], current_length_data[key]) = initialize_past_key_values(self.base_model)
                    self.past_key_values[key] = past_key_values[key]
                    self.past_key_values_data[key] = past_key_values_data[key]
                    self.current_length_data[key] = current_length_data[key]

        input_len = input_ids.shape[1]
        self.reset_tree_mode()

        # compute the attention mask for drafter (for sequential CFG mode) or both (for parallel CFG mode)
        self.image_start_token_id_index = torch.where(input_ids[0] == self.image_start_token_id)[0][-1].item()
        prompt_length = torch.where(input_ids == self.image_start_token_id)[-1].item()
        num_image_tokens = input_ids.shape[1] - prompt_length # designed to be 3

        zero_padding = torch.zeros((prompt_length), dtype=torch.bool, device=input_ids.device)
        cond_attn_mask = torch.ones((input_ids.shape[1]), dtype=torch.bool, device=input_ids.device)
        uncond_attn_mask = torch.cat((zero_padding, torch.ones((num_image_tokens), dtype=torch.bool, device=input_ids.device)), dim=-1)
        attn_mask = torch.stack([cond_attn_mask, uncond_attn_mask], dim=0)

        if self.cfg_mode == "parallel":
            input_ids = input_ids.repeat(2, 1)
        
        if self.eagle_version == 1:
            tree_logits, sample_token = self.initialize_tree(
                input_ids=input_ids,
                attention_mask=attn_mask,
                tree_attn_mask=tree_buffers["tree_attn_mask"],
                past_key_values=past_key_values,
                logits_processors=logits_processors,
            )
            tree_position_ids = tree_buffers["tree_position_ids"]
            retrieve_indices = tree_buffers["retrieve_indices_head"]

        else:
            tree_candidates, retrieve_indices, tree_mask, tree_position_ids = self.initialize_tree(
                input_ids=input_ids,
                attention_mask=attn_mask,
                past_key_values=past_key_values,
                logits_processors=logits_processors,
            )
            if self.cfg_mode == "parallel":
                tree_mask = tree_mask.repeat(2, 1, 1, 1)

        new_token = 0
        pbar = tqdm(total=max_new_tokens)
        while new_token < max_new_tokens:
            if self.eagle_version == 1:
                candidates, cart_candidates_prob, tree_candidates = self.generate_candidates(
                    tree_logits=tree_logits,
                    tree_indices=tree_buffers["tree_indices"],
                    retrieve_indices=tree_buffers["retrieve_indices"],
                    sample_token=sample_token
                )
            else:
                self.base_model.model.tree_mask = tree_mask
                tree_candidates = tree_candidates.to(input_ids.device)
            
            logits, hidden_states_new, uncond_hidden_states_new = self.tree_decoding(
                tree_candidates=tree_candidates,
                attention_mask=attn_mask,
                past_key_values=past_key_values,
                tree_position_ids=tree_position_ids,
                input_ids=input_ids,
                retrieve_indices=retrieve_indices,
            )
            
            if self.eagle_version == 1:
                original_prob = tree_logits[2]
                p_indices = tree_buffers["p_indices"]
                b_indices = tree_buffers["b_indices"]
            else:
                padding = (torch.zeros(1, 1, dtype=torch.long) - 1).to(input_ids.device)
                tree_candidates = torch.cat((tree_candidates, padding), dim=1)
                candidates = tree_candidates[0, retrieve_indices]
                cart_candidates_prob = None
                original_prob = None
                p_indices = None
                b_indices = None

            best_candidate, accept_length, sample_p = self.evaluate_posterior(
                logits=logits,
                candidates=candidates,
                cart_candidates_prob=cart_candidates_prob,
                original_prob=original_prob,
                tree_candidates=tree_candidates,
                p_indices=p_indices,
                b_indices=b_indices,
                do_sample=do_sample,
                lantern=lantern,
                lantern_k=lantern_k,
                lantern_delta=lantern_delta,
            )

            input_ids, output, new_token, sample_token = self.update_inference_inputs(
                input_ids=input_ids,
                attention_mask=attn_mask,
                candidates=candidates,
                best_candidate=best_candidate,
                accept_length=accept_length,
                retrieve_indices=retrieve_indices,
                do_sample=do_sample,
                new_token=new_token,
                past_key_values_data=past_key_values_data,
                current_length_data=current_length_data,
                hidden_states_new=hidden_states_new,
                uncond_hidden_states_new=uncond_hidden_states_new,
                sample_p=sample_p
            )

            if self.eagle_version == 1:
                tree_logits = output
            else:
                tree_candidates, retrieve_indices, tree_mask, tree_position_ids = output
                if self.cfg_mode == "parallel":
                    tree_mask = tree_mask.repeat(2, 1, 1, 1)
            
            accept_length_list.append(accept_length+1)
            pbar.update(accept_length+1)

            if eos_token_ids is not None:
                if eos_token_ids in input_ids[0, input_len:].tolist():
                    break

            if input_ids.shape[1] > max_length:
                break
        pbar.close()
        
        return input_ids, accept_length_list