
# LANTERN

This repository is an official PyTorch implementation of the paper [LANTERN: Accelerating Visual Autoregressive Models via Relaxed Speculative Decoding](https://arxiv.org/abs/2410.03355) (ICLR 2025) and [LANTERN++: Enhanced Relaxed Speculative Decoding with Static Tree Drafting for Visual Auto-regressive Models](https://arxiv.org/abs/2502.06352) (ICLRW - SCOPE(Oral) 2025), which supports various functionalities related to LANTERN, including model inference, drafter model training, drafter model training data generation and image decoding for image generation.

---

## ğŸ“° News

- **[2025-03-05] ğŸ‰ğŸ‰ğŸ‰ LANTERN is released! ğŸ‰ğŸ‰ğŸ‰**

---

## ğŸ“š Table of Contents
1. [Directory Structure](#directory-structure)
2. [Installation](#installation)
3. [Key Features](#key-features)
4. [Usage](#usage)
5. [License](#license)
6. [Acknowledgement](#acknowledgement)
7. [Citation](#citation)
---

## ğŸ—‚ï¸ Directory Structure

The main directory structure of the project is as follows:

```
.
â”œâ”€â”€ models/                                     # Model and related modules
â”‚   â”œâ”€â”€ base_models/                            # Base model modules
â”‚   â”‚   â”œâ”€â”€ lumina_mgpt
â”‚   â”‚   â”‚   â”œâ”€â”€ modeling_lumina_mgpt.py
â”‚   â”‚   â”‚   â””â”€â”€ other files...
â”‚   â”‚   â””â”€â”€ other models...     
â”‚   â”œâ”€â”€ kv_variants/                            # Key-Value variant models
â”‚   â”‚   â”œâ”€â”€ modeling_lumina_mgpt_kv.py
|   |   â””â”€â”€ modeling_anole_kv.py
â”‚   â”‚   â””â”€â”€ other models...
â”‚   â”œâ”€â”€ drafters/                               # Drafter model modules
â”‚   â”‚   â”œâ”€â”€ kv_cache.py
â”‚   â”‚   â”œâ”€â”€ choices
â”‚   â”‚   â”œâ”€â”€ cnets_lumina_mgpt.py
|   |   â”œâ”€â”€ cnets_anole.py
â”‚   â”‚   â”œâ”€â”€ cnets_{other_models}.py ...   
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ configs/                                # Configuration modules
â”‚   â”‚   â”œâ”€â”€ configs.py
â”‚   â”‚   â”œâ”€â”€ configuration_lumina_mgpt.py
|   |   â”œâ”€â”€ configuration_anole.py
â”‚   â”‚   â””â”€â”€ configuration_{other_models}.py...
â”‚   â”œâ”€â”€ ea_model_lumina_mgpt.py                 # EAGLE models
|   â”œâ”€â”€ ea_model_anole.py
â”‚   â””â”€â”€ ea_model_{other_models}.py...
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â”œâ”€â”€ lumina_mgpt_config.json             # Configuration for model init
|   |   â”œâ”€â”€ anole_config.json
â”‚   â”‚   â””â”€â”€ configs for other models...
â”‚   â”œâ”€â”€ prompts/                                # Prompts for image generation
â”‚   â”œâ”€â”€ self_distilled_data/                    # Self-distilled data for drafter training
â”‚   â””â”€â”€ drafter_train_data/                     # Train data for drafter
â”œâ”€â”€ ckpts/                                      # Model checkpoints folder
â”‚   â”œâ”€â”€ lumina_mgpt/
â”‚   â”‚   â”œâ”€â”€ chameleon/                      
â”‚   â”‚   â”œâ”€â”€ Lumina-mGPT-7B-768/                 # Model and tokenizer files
â”‚   â”‚   â”œâ”€â”€ trained_drafters/                   # Trained drafter models
|   |   |   â””â”€â”€...state_20/
|   |   |      â”œâ”€â”€ config.json                  # config.json for drafter model
|   |   |      â””â”€â”€ other files...
â”‚   â”‚   â””â”€â”€ vq_distances/                       # Pre-computed VQ distances for LANTERN
â”‚   â””â”€â”€ other models...
â”œâ”€â”€ entrypoints/                                # Execution entry points
â”‚   â”œâ”€â”€ train_drafter/
â”‚   â”‚   â”œâ”€â”€ data_utils.py
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ generate_codebook.py
â”‚   â”œâ”€â”€ generate_images.py
â”‚   â”œâ”€â”€ generate_train_data.py
â”‚   â””â”€â”€ other files...
â”œâ”€â”€ third_party/                                # Third-party libraries
â”‚   â””â”€â”€ vllm
â”œâ”€â”€ main.py                                     # Main execution script
â”œâ”€â”€ requirements.txt                            # Project dependencies
â”œâ”€â”€ environment.yaml
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

Here is a brief description for each directory.
1. **`models/`** - Contains model implementations and related modules.
   - **`base_models/`** - Base model implementations (e.g., Lumina-mGPT, LlamaGen, Anole).
   - **`kv_variants/`** - Modified base models with Key-Value cache adaptations for enhanced compatibility with EAGLEâ€™s architecture.
   - **`drafters/`** - Modules and auxiliary code for drafter models.
   - **`configs/`** - Configuration modules for each model (e.g., `ChameleonConfig` for Lumina-mGPT).

2. **`data/`** - Stores configuration files, text prompts, self-distilled data, and drafter training data.

3. **`ckpts/`** - Checkpoints for all models, including trained drafters and VQ distances for relaxed speculative decoding.

4. **`entrypoints/`** - Primary scripts for tasks such as image generation, codebook generation, and drafter training.

5. **`third_party/`** - Custom external libraries, including modifications for specific functionality.

---

## âš™ï¸ Installation

1. **Install Required Packages**
    **Requirements**
    - Python >= 3.10
    - PyTorch >= 2.4.0
    
    Install the dependencies listed in `requirements.txt`.
    ```bash
    git clone https://github.com/jadohu/LANTERN
    cd LANTERN
    pip install -r requirements.txt
    ```

2. **Additional Setup**
    1. **Lumina-mGPT**
        For [Lumina-mGPT](https://github.com/Alpha-VLLM/Lumina-mGPT), we need to install `flash_attention` and `xllmx` packages.
        ```bash
        pip install flash-attn --no-build-isolation
        cd models/base_models/lumina_mgpt
        pip install -e .
        ```
        1. **(Optional) vLLM**
            Install and set up [`vLLM`](https://github.com/vllm-project/vllm) with the required modifications. Note that we use `vLLM==0.6.3` and build from source. The required modifications are specifed in `third_party/vllm`. The installation procedure is as follows.
            ```bash
            pip install https://vllm-wheels.s3.us-west-2.amazonaws.com/fd47e57f4b0d5f7920903490bce13bc9e49d8dba/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl
            git clone https://github.com/vllm-project/vllm
            cd vllm
            git checkout tags/v0.6.3

            cd ..
            mv -rf third_party/vllm/* vllm/
            cd vllm
            python python_only_dev.py
            ```

3. **Checkpoints**
    All model weights and other required data should be stored in `ckpts/`.
    1. **Lumina-mGPT**
        For Lumina-mGPT, since currently the Chameleon implementation in transformers does not contain the VQ-VAE decoder, please manually download the original VQ-VAE weights [provided by Meta](https://github.com/facebookresearch/chameleon) and put them to the following directory:
        ```
        ckpts
        â””â”€â”€ lumina_mgpt
            â””â”€â”€ chameleon
                â””â”€â”€ tokenizer
                    â”œâ”€â”€ text_tokenizer.json
                    â”œâ”€â”€ vqgan.yaml
                    â””â”€â”€ vqgan.ckpt
        ```

        Also download the original model [`Lumina-mGPT-7B-768`](https://huggingface.co/Alpha-VLLM/Lumina-mGPT-7B-768) from Huggingface ğŸ¤— and put them to the following directory:
        ```
        ckpts
        â””â”€â”€ lumina_mgpt
            â””â”€â”€ Lumina-mGPT-7B-768
                â”œâ”€â”€ config.json
                â”œâ”€â”€ generation_config.json
                â”œâ”€â”€ model-00001-of-00002.safetensors
                â””â”€â”€ other files...
        ```
    2. **LlamaGen**
        For LlamaGen T2I model, download [`LlamaGen-T2I`](https://huggingface.co/jadohu/LlamaGen-T2I) and/or [`LlamaGen-T2I-2`](https://huggingface.co/jadohu/LlamaGen-T2I-2), which is a huggingface style converted model from [`LlamaGen`](https://github.com/FoundationVision/LlamaGen). 
        
        In addition, you should download [`VQ-VAE`](https://huggingface.co/peizesun/llamagen_t2i/resolve/main/vq_ds16_t2i.pt) and [`flan-t5-xl`](https://huggingface.co/google/flan-t5-xl).

        ```
        ckpts
        â””â”€â”€ llamagen
            â”œâ”€â”€ LlamaGen-T2I
            â”‚   â”œâ”€â”€ config.json
            â”‚   â”œâ”€â”€ generation_config.json
            â”‚   â”œâ”€â”€ model.safetensors
            â”‚   â””â”€â”€ other files...
            â”œâ”€â”€ LlamaGen-T2I-2
            â”‚   â”œâ”€â”€ config.json
            â”‚   â”œâ”€â”€ generation_config.json
            â”‚   â”œâ”€â”€ model.safetensors
            â”‚   â””â”€â”€ other files...
            â”œâ”€â”€ vq_ds16_t2i.pt
            â””â”€â”€ t5
                â””â”€â”€ flan-t5-xl
                    â”œâ”€â”€ config.json
                    â”œâ”€â”€ generation_config.json
                    â”œâ”€â”€ model-00001-of-00002.safetensors
                    â””â”€â”€ other files...
        ```

        **(Optional) Trained drafter**
        To use trained drafter, you need to download [`llamagen_drafter`](https://huggingface.co/jadohu/llamagen_drafter) and/or [`llamagen2_drafter`](https://huggingface.co/jadohu/llamagen2_drafter) and save it under trained_drafters directory.
        ```
        ckpts
        â””â”€â”€ llamagen
            â””â”€â”€ trained_drafters
                â”œâ”€â”€ llamagen_drafter
                |   â”œâ”€â”€ config.json
                |   â”œâ”€â”€ generation_config.json
                |   â”œâ”€â”€ pytorch_model.bin
                |   â””â”€â”€ other files...
                â””â”€â”€ llamagen2_drafter
                    â”œâ”€â”€ config.json
                    â”œâ”€â”€ generation_config.json
                    â”œâ”€â”€ pytorch_model.bin
                    â””â”€â”€ other files...
        ```
    3. **Anole**
        For Anole, download [`Anole-7b-v0.1-hf`](https://huggingface.co/leloy/Anole-7b-v0.1-hf), which is a huggingface style converted model from [`Anole`](https://huggingface.co/GAIR/Anole-7b-v0.1). 
        
        In addition, you should download the original VQ-VAE weights [provided by Meta](https://github.com/facebookresearch/chameleon) and put them to the following directory:

        ```
        ckpts
        â””â”€â”€ anole
            â”œâ”€â”€ Anole-7b-v0.1-hf
            |   â”œâ”€â”€ config.json
            |   â”œâ”€â”€ generation_config.json
            |   â”œâ”€â”€ model-00001-of-00003.safetensors
            |   â””â”€â”€ other files...
            â””â”€â”€ chameleon
                â””â”€â”€ tokenizer
                    â”œâ”€â”€ text_tokenizer.json
                    â”œâ”€â”€ vqgan.yaml
                    â””â”€â”€ vqgan.ckpt
        ```

        **(Optional) Trained drafter**
        To use trained drafter, you need to download [`anole_drafter`](https://huggingface.co/jadohu/anole_drafter) and save it under trained_drafters directory.
        ```
        ckpts
        â””â”€â”€ anole
            â””â”€â”€ trained_drafters
                â””â”€â”€ anole_drafter
                    â”œâ”€â”€ config.json
                    â”œâ”€â”€ generation_config.json
                    â”œâ”€â”€ pytorch_model.bin
                    â””â”€â”€ other files...
        ```

---

## âœ¨ Usage

All the functionalities can be done by either running `main.py` or directly running `entrypoints/{function}.py`.
Currently, "llamagen" (LlamaGen-Stage I), "llamagen2" (LlamaGen-Stage II), "anole", and "lumina_mgpt" are supported as --model.

ğŸš§ **Lumina-mGPT is still under construction, so some functions may not work properly yet. You can follow the procedures here, but you may encounter a few exceptions.**

1. **Generate Images**
    ```bash
    python main.py generate_images --model <model_name> --model_type <model_type; e.g., base, vllm, eagle> --model_path <model_path> --drafter_path <drafter_path> --output_dir <output_dir> ...
    ```
    or
     ```bash
     python -m entrypoints.generate_images --model <model_name> --model_type <model_type; e.g., base, vllm, eagle> --model_path <model_path> --drafter_path <drafter_path> --output_dir <output_dir> ...
     ```

    ğŸ’¡**How to use LANTERN and LANTERN++ for image generation**
    - For **LANTERN**, set `--model_type eagle`, turn on `--lantern` option and set `--lantern_k` and `--lantern_delta` options.
    - For **LANTERN++**, use `--static_tree` option and use `--lantern_delta` to set $\lambda$ value. 

2. **Generate Training Data for Drafter**
    ```bash
    python main.py generate_train_data --model <model_name> --data_path <path_to_image_tokens> --output_dir <output_dir> --num_samples <num_samples>
    ```
    or
     ```bash
     python -m entrypoints.generate_train_data --model <model_name> --data_path <path_to_image_tokens> --output_dir <output_dir> --num_samples <num_samples>
     ```

    For **LlamaGen** and **Anole**, you have to extract code and T5 embedding(only for LlamaGen) for training data. 
    - Locate image and caption files in given format and execute following command before run **generate_train_data**:

    **Data Format:**
    - image_folder
        - {file_1}.jpg
        - {file_1}.txt
        - {file_2}.jpg
        - {file_2}.txt
        ... 
    ```bash
    python main.py extract_code --model <model_type> --data_path <path_to_image_and_caption> --output_dir <output_dir> --num_samples <num_samples>
    ```
    or 
    ```bash
    python -m entrypoints.extract_code --model <model_type> --data_path <path_to_image_and_caption> --output_dir <output_dir> --num_samples <num_samples>
    ```
3. **Train Drafter Model**
   ```bash
    python main.py train_drafter --model <model_type> --base_path <base_model_path> --config_path <path_to_config.json> --data_dir <data_dir> --save_dir <save_dir> --lr <lr> --bs <bs> --gradient_accumlation_steps <gradient_accumulation_steps> ...
    ```
    or
     ```bash
     python -m entrypoints.train_drafter.main --model <model_type> --base_path <base_model_path> --config_path <path_to_config.json> --data_dir <data_dir> --save_dir <save_dir> --lr <lr> --bs <bs> --gradient_accumlation_steps <gradient_accumulation_steps> ...
     ```

    For multi GPU training with accelerate, you can use
    ```bash
     accelerate launch -m entrypoints.train_drafter.main --model <model_type> --base_path <base_model_path> --config_path <path_to_config.json> --data_dir <data_dir> --save_dir <save_dir> --lr <lr> --bs <bs> --gradient_accumlation_steps <gradient_accumulation_steps> ...
     ```

4. **Generate VQ Distances**
     ```bash
     python main.py generate_codebook --model <model_name> --save_path <save_path>
     ```
     or
     ```bash
     python -m entrypoints.generate_codebook --model <model_name> --save_path <save_path>
     ```


5. **Evaluate Generated Images**
    We support FID, CLIP score, Precision/Recall and HPSv2 for image evaluation.
    ```bash
    python main.py eval_fid_clip --fake_dir <path_to_generated_image> --ref_dir <path_to_reference_image> --caption_path <path_to_prompt> --how_many <number_of_images_for_evaluation> ...
    ```
    ```bash
    python main.py eval_prec_recall --fake_dir <path_to_generated_image> --ref_dir <path_to_reference_image> ...
    ```
    ```bash
    python main.py eval_hpsv2 --image_path <path_to_generated_image> --prompt_path <path_to_prompt>
    ```
    or 
    ```bash
    python -m entrypoints.eval_fid_clip --fake_dir <path_to_generated_image> --ref_dir <path_to_reference_image> --caption_path <path_to_prompt> --how_many <number_of_images_for_evaluation> ...
    ```
    ```bash
    python -m entrypoints.eval_prec_recall --fake_dir <path_to_generated_image> --ref_dir <path_to_reference_image> ...
    ```
    ```bash
    python -m entrypoints.eval_hpsv2 --image_path <path_to_generated_image> --prompt_path <path_to_prompt>
    ```
---

## âš ï¸ CAUTIONS
1. **`config.json` should be in the `ckpts/{model_name}/trained_models/{drafter_path}`**
    Since the `Model` in `cnets_{model_name}.py` is initialized according to the `config.json` in the `drafter_path`, you need to place `config.json` for drafter correctly. Note that the `config.json` should be same as the base model's `config.json` other than `num_hidden_layers`.

---

## âš–ï¸ License

This project is distributed under the Chameleon License by Meta Platforms, Inc. For more information, please see the `LICENSE` file in the repository.

---

## ğŸ™ Acknowledgement
This repository is built with extensive reference to [FoundationVision/LlamaGen](https://github.com/FoundationVision/LlamaGen), [Alpha-VLLM/Lumina-mGPT](https://github.com/Alpha-VLLM/Lumina-mGPT) and [SafeAILab/EAGLE](https://github.com/SafeAILab/EAGLE), leveraging many of their core components and approaches.

---

## ğŸ“„ Citation

```
@article{jang2024lantern,
  title={LANTERN: Accelerating Visual Autoregressive Models with Relaxed Speculative Decoding},
  author={Jang, Doohyuk and Park, Sihwan and Yang, June Yong and Jung, Yeonsung and Yun, Jihun and Kundu, Souvik and Kim, Sung-Yub and Yang, Eunho},
  journal={arXiv preprint arXiv:2410.03355},
  year={2024}
}
@article{park2025lanternenhancedrelaxedspeculative,
  title={LANTERN++: Enhanced Relaxed Speculative Decoding with Static Tree Drafting for Visual Auto-regressive Models}, 
  author={Sihwan Park and Doohyuk Jang and Sungyub Kim and Souvik Kundu and Eunho Yang},
  journal={arXiv preprint arXiv:2410.03355},
  year={2025}
}
```